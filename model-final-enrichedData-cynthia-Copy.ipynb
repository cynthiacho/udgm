{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import pydot_ng as pydot\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from keras.models import Sequential, Model\n",
    "#from keras.layers import Input, Dense, Activation, Reshape\n",
    "#from keras.layers import Concatenate, Dropout\n",
    "#from keras.layers.embeddings import Embedding\n",
    "#from keras.utils import plot_model\n",
    "#from keras.optimizers import SGD\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"./data/TrainingSet.csv\",index_col=0)\n",
    "submission_labels = pd.read_csv(\"./data/SubmissionRows.csv\", index_col=0)\n",
    "\n",
    "training_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Â Func to split that dataframe to values [2005,2006,2007] and [country_name,series_code,series_name]\n",
    "def split_dataframe(data):\n",
    "    raw_data = data.loc[:,:'2007 [YR2007]']\n",
    "    #raw_data=data.iloc[:,:-1] \n",
    "    description = data.loc[:,'Country Name':]\n",
    "    #description=data.iloc[:,-1] \n",
    "    return raw_data,description\n",
    "\n",
    "\n",
    "raw_data, description = split_dataframe(training_data)\n",
    "\n",
    "description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values using standard interpolation method\n",
    "\n",
    "raw_data = raw_data.interpolate(limit_direction='both',axis=1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=raw_data.join(description)\n",
    "\n",
    "training_data.head(1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to interpolate missing data  and ensure there are no nulls\n",
    "\n",
    "#training_data_pivotted = training_data.melt(id_vars=['Series Code', 'Country Name'], var_name='Year', value_name='indicator',ignore_index = False, value_vars=['2005 [YR2005]','2006 [YR2006]','2007 [YR2007]'])\n",
    "#df=training_data_pivotted[training_data_pivotted.indicator.isnull()]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_pivotted = training_data.melt(id_vars=['Series Code', 'Country Name'], var_name='Year', value_name='indicator',ignore_index = False, \\\n",
    "                        value_vars=['1972 [YR1972]','1973 [YR1973]','1974 [YR1974]','1975 [YR1975]',\\\n",
    "                                    '1976 [YR1976]','1977 [YR1977]','1978 [YR1978]','1979 [YR1979]', \\\n",
    "                                    '1980 [YR1980]',  '1981 [YR1981]', '1982 [YR1982]', '1983 [YR1983]', \\\n",
    "                                    '1984 [YR1984]','1985 [YR1985]','1986 [YR1986]','1987 [YR1987]', \\\n",
    "                                    '1988 [YR1988]','1989 [YR1989]','1990 [YR1990]','1991 [YR1991]', \\\n",
    "                                    '1992 [YR1992]','1993 [YR1993]','1994 [YR1994]','1995 [YR1995]', \\\n",
    "                                    '1996 [YR1996]', '1997 [YR1997]','1998 [YR1998]','1999 [YR1999]', \\\n",
    "                                    '2000 [YR2000]','2001 [YR2001]','2002 [YR2002]','2003 [YR2003]' , \\\n",
    "                                    '2004 [YR2004]','2005 [YR2005]','2006 [YR2006]','2007 [YR2007]'])\n",
    "\n",
    "\n",
    "'''\n",
    "training_data_pivotted = training_data.melt(id_vars=['Series Code', 'Country Name'], var_name='Year', value_name='indicator',ignore_index = False, value_vars=['2005 [YR2005]','2006 [YR2006]','2007 [YR2007]'])\n",
    "'''\n",
    "\n",
    "training_data_pivotted['Year'] = training_data_pivotted['Year'].str.extract('(\\d+)', expand=False) #.4stype(i4t),'2005 [YR2005]','2006 [YR2006]'\n",
    "\n",
    "\n",
    "# Let try to interpolate the missing values \n",
    "\n",
    "\n",
    "#df = df.sort_values(['Week', 'Product_Code'])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "training_data_pivotted['Year'] = pd.to_datetime(training_data_pivotted['Year'], format='%Y')\n",
    "\n",
    "#Ignore data is null \n",
    "\n",
    "#df=df[df.indicator.notnull()]\n",
    "\n",
    "#df.head()\n",
    "\n",
    "#training_data_pivotted.to_csv(\"./data/training_data_pivotted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imputed_results_raw=pd.read_csv(\"../data/imputed_res.csv\",index_col='int64_field_0')\n",
    "\n",
    "imputed_results_raw=training_data_pivotted\n",
    "\n",
    "# Only fetch the row required for prediction as presence of other indicators causes data skew problems \n",
    "\n",
    "imputed_results=imputed_results_raw.join(submission_labels,how='inner')\n",
    "\n",
    "imputed_results=imputed_results.rename({'Series_Code':'Series Code','Country_Name': 'Country Name'},axis=1)\n",
    "\n",
    "imputed_results=imputed_results.drop(['2008 [YR2008]','2012 [YR2012]'], axis=1)\n",
    "\n",
    "#imputed_results.groupby('Year').nunique()\n",
    "\n",
    "#make sure the index is preserved. May not be required\n",
    "#imputed_results[imputed_results.index == 559]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min:: \" + str(imputed_results[\"indicator\"].min()) + \"\\nmax:: \" + str(imputed_results[\"indicator\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure null values are imputed \n",
    "df=imputed_results[imputed_results.indicator.isnull()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the number of years you want to compute it for \n",
    "imputed_results=imputed_results[pd.to_datetime(imputed_results['Year'], format='%Y-%m-%d') >= '1990-01-01']\n",
    "#imputed_results=imputed_results[imputed_results['Country Name'] == 'Afghanistan']\n",
    "imputed_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Enrichment \n",
    "# Append the continent and sub region \n",
    "countryContinent= pd.read_csv(\"./data/countryContinentutf8.csv\")\n",
    "\n",
    "countryContinent=countryContinent.fillna(0)\n",
    "\n",
    "countryContinent=countryContinent.rename({'country':'Country Name'},axis=1)\n",
    "\n",
    "countryContinent[countryContinent.region_code.isnull()]\n",
    "\n",
    "#countryContinent.groupby(\"Country Name\").nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed_results_joined=imputed_results.join(countryContinent,on=['Country Name'],how='left')\n",
    "imputed_results= pd.merge(left=imputed_results, right=countryContinent, how='left', left_on='Country Name', right_on='Country Name')\n",
    "\n",
    "imputed_results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure null values are imputed \n",
    "imputed_results=imputed_results.fillna(0)\n",
    "\n",
    "df=imputed_results[imputed_results.region_code.isnull()]\n",
    "df.groupby(\"Country Name\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply encoding of categorical values \n",
    "# Label Encoding \n",
    "\n",
    "encode_columns = ['Country Name','Series Code']\n",
    "encode_df = imputed_results[encode_columns]\n",
    "encode_df = encode_df.astype('str')\n",
    "encode_df = encode_df.apply(LabelEncoder().fit_transform)\n",
    "score_encode_drop = imputed_results.drop(encode_columns, axis = 1)\n",
    "#score_encode_drop=df\n",
    "\n",
    "score_encode = pd.concat([score_encode_drop, encode_df], axis = 1)\n",
    "score_encode['Year']=pd.to_datetime(imputed_results['Year'], format='%Y-%m-%d').dt.year\n",
    "score_target=score_encode\n",
    "#score_target.groupby('Country Name').nunique()\n",
    "score_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train and Test Data\n",
    "split_date = 2006\n",
    "df_train = score_target.loc[score_target['Year'] <= split_date].copy()\n",
    "df_test = score_target.loc[score_target['Year'] > split_date].copy()\n",
    "\n",
    "def create_features(df, label=None):\n",
    "    #df['Year'] =  pd.to_datetime(df['Year'], format='%Y').dt.year\n",
    "    \n",
    "    #Original config without the enrichment \n",
    "    #X = df[['Country Name','Series Code','Year']]  \n",
    "    \n",
    "    #With enrichment changing the features\n",
    "    X = df[['country_code','Series Code','region_code','sub_region_code', 'Year']] \n",
    "    \n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X\n",
    "X_train, y_train = create_features(df_train, label='indicator')\n",
    "X_test, y_test = create_features(df_test, label='indicator')\n",
    "\n",
    "#X_train.count()\n",
    "X_train.head()\n",
    "#X_train.groupby('Year').nunique()\n",
    "#print(create_features(df_train, label='indicator'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BELOW-CYNTHIA ADDED ALGORITHM MLkNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "classifier_new = MLkNN(k=10)\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "x_train = lil_matrix(x_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(x_test).toarray()\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions_new))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test with simple Regression Model \n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print ('RMSE:', rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xboost Model\n",
    "\n",
    "params = {'colsample_bytree': uniform(0.7, 0.3),\n",
    "          'gamma': uniform(0, 0.7),\n",
    "          'learning_rate': uniform(0.003, 0.5), # default 0.1 \n",
    "          'max_depth': randint(60, 100), # default 3\n",
    "          'n_estimators': randint(9999, 10000), # default 100\n",
    "          'subsample': uniform(0.6, 0.4)}\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\",random_state=300)\n",
    "time_split = TimeSeriesSplit(n_splits = 2)\n",
    "xgb_search = RandomizedSearchCV(xgb_model,param_distributions=params, random_state=300, n_iter=4, cv=time_split, verbose=1, n_jobs=1, return_train_score=True)\n",
    "xgb_search.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xgb_search.predict(X_test)\n",
    "#y_pred = model.predict(X_test)\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print ('RMSE:', rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest Model\n",
    "\n",
    "#n_estimators=[500, 800,1500,2500,5000]\n",
    "#max_features=['auto','sqrt','log2']\n",
    "#max_depth= [10,20,30,40,50]\n",
    "#max_depth.append(None)\n",
    "#min_samples_split=[2,5,10,15,20]\n",
    "#min_samples_leaf=[1,2,5,10,15]\n",
    "\n",
    "n_estimators=[1000]\n",
    "max_features=['auto']\n",
    "max_depth= [10]\n",
    "max_depth.append(None)\n",
    "min_samples_split=[2]\n",
    "min_samples_leaf=[1]\n",
    "\n",
    "\n",
    "\n",
    "grid_param = {'n_estimators': n_estimators, \\\n",
    "             'max_features': max_features, \\\n",
    "             'max_depth': max_depth, \\\n",
    "             'min_samples_split': min_samples_split, \\\n",
    "             'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "'''\n",
    "model = RandomForestRegressor(n_estimators=1000,random_state=100,max_features='sqrt')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "'''\n",
    "model = RandomForestRegressor(random_state=1)\n",
    "model_random=RandomizedSearchCV(estimator=model,param_distributions=grid_param, n_iter=500, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "model_random.fit(X_train,y_train)\n",
    "print(model_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=1000,random_state=1,max_features='auto',max_depth=100, min_samples_split=2,min_samples_leaf=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print ('RMSE:', rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2005 Split  + 1972 data \n",
    "# RMSE: 0.04403392579660952\n",
    "\n",
    "#2006 Split + 1972 data\n",
    "# RSME :  0.03701239963313429\n",
    "\n",
    "#2006 Split + 2000 data \n",
    "#0.0367124955614689\n",
    "\n",
    "#200 estimators, random 100\n",
    "#RMSE: 0.038374986721595734\n",
    "\n",
    "#RMSE: 0.03692818441424601\n",
    "\n",
    "\n",
    "#With Enriched Data \n",
    "# 0.06533870558142173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_results=submission_labels.join(training_data)\n",
    "\n",
    "submission_results=submission_results[['Country Name','Series Code']]\n",
    "\n",
    "submission_results['Year']=2008\n",
    "\n",
    "submission_results['indicator']=0\n",
    "\n",
    "#submission_results.head() \n",
    "\n",
    "#submission_results.groupby('Year').nunique()\n",
    "\n",
    "# Label Encoding \n",
    "\n",
    "encode_columns = ['Series Code']\n",
    "pred_df = submission_results[encode_columns]\n",
    "score_predict_drop = submission_results.drop(encode_columns, axis = 1)\n",
    "pred_df = pred_df.astype('str')\n",
    "pred_df = pred_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "score_predict_2008 = pd.concat([score_predict_drop, pred_df], axis = 1)\n",
    "\n",
    "score_predict_2008['Year']=pd.to_datetime(score_predict_2008['Year'],format='%Y').dt.year\n",
    "\n",
    "#res_pred_01 = xgb_search.predict(score_predict_2008[['Country Name','Series Code','Year']])\n",
    "\n",
    "\n",
    "score_predict_2008=pd.merge(left=score_predict_2008, right=countryContinent, how='left', left_on='Country Name', right_on='Country Name').set_axis(score_predict_2008.index)\n",
    "\n",
    "\n",
    "score_predict_2008=score_predict_2008.fillna(0)\n",
    "\n",
    "\n",
    "#Replace the model name here for different models \n",
    "\n",
    "#Random forest\n",
    "#res_pred_01 = model.predict(score_predict_2008[['country_code','Series Code','region_code','sub_region_code', 'Year']])\n",
    "\n",
    "#Xboost\n",
    "res_pred_01 = xgb_search.predict(score_predict_2008[['country_code','Series Code','region_code','sub_region_code', 'Year']])\n",
    "\n",
    "score_predict_2008['predictions'] = res_pred_01\n",
    "\n",
    "score_predict_2008.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_results=submission_labels.join(training_data)\n",
    "\n",
    "submission_results=submission_results[['Country Name','Series Code']]\n",
    "\n",
    "submission_results['Year']=2012\n",
    "\n",
    "submission_results['indicator']=0\n",
    "\n",
    "#submission_results.head() \n",
    "\n",
    "#submission_results.groupby('Year').nunique()\n",
    "\n",
    "# Label Encoding \n",
    "\n",
    "encode_columns = ['Series Code']\n",
    "pred_df = submission_results[encode_columns]\n",
    "score_predict_drop = submission_results.drop(encode_columns, axis = 1)\n",
    "pred_df = pred_df.astype('str')\n",
    "pred_df = pred_df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "score_predict_2012 = pd.concat([score_predict_drop, pred_df], axis = 1)\n",
    "\n",
    "score_predict_2012['Year']=pd.to_datetime(score_predict_2012['Year'],format='%Y').dt.year\n",
    "\n",
    "#res_pred_01 = xgb_search.predict(score_predict_2008[['Country Name','Series Code','Year']])\n",
    "\n",
    "\n",
    "score_predict_2012=pd.merge(left=score_predict_2012, right=countryContinent, how='left', left_on='Country Name', right_on='Country Name').set_axis(score_predict_2012.index)\n",
    "\n",
    "\n",
    "score_predict_2012=score_predict_2012.fillna(0)\n",
    "\n",
    "#Random forest\n",
    "#res_pred_01 = model.predict(score_predict_2012[['country_code','Series Code','region_code','sub_region_code', 'Year']])\n",
    "\n",
    "\n",
    "#xgboost\n",
    "res_pred_01 = xgb_search.predict(score_predict_2012[['country_code','Series Code','region_code','sub_region_code', 'Year']])\n",
    "\n",
    "\n",
    "score_predict_2012['predictions'] = res_pred_01\n",
    "\n",
    "score_predict_2012.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CYNTHIA ADDED DATA VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "# Declaring the figure or the plot (y, x) or (width, height)\n",
    "plt.figure(figsize = (12,7))\n",
    "# Categorical data: Country names\n",
    "Predictors = ['Year', 'Indicator', 'Country Name', 'Series Code']\n",
    "# Integer value interms of death counts\n",
    "Predictions = [0.634903, 0.055296 -0.028926, -0.028926, 0.798667]\n",
    "# Passing the parameters to the bar function, this is the main function which creates the bar plot\n",
    "plt.bar(Predictors, Predictions, width= 0.9, align='center',color='cyan', edgecolor = 'red')\n",
    "# This is the location for the annotated text\n",
    "i = 1.0\n",
    "j = 25\n",
    "# Annotating the bar plot with the values (total death count)\n",
    "for i in range(len(Predictors)):\n",
    "    plt.annotate(Predictions[i], (-0.1 + i, Predictions[i] + j))\n",
    "# Creating the legend of the bars in the plot\n",
    "plt.legend(labels = ['Predictions'])\n",
    "# Giving the tilte for the plot\n",
    "plt.title(\"Bar plot representing the total deaths by top 6 countries due to coronavirus\")\n",
    "# Namimg the x and y axis\n",
    "plt.xlabel('Predictors')\n",
    "plt.ylabel('Predictions')\n",
    "# Saving the plot as a 'png'\n",
    "plt.savefig('1BarPlot.png')\n",
    "# Displaying the bar plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_from_model=score_predict_2012.join(score_predict_2008,how='inner',lsuffix=2012, rsuffix=2008)\n",
    "\n",
    "\n",
    "predictions_from_model=predictions_from_model.drop(columns=['Year2012','indicator2012','Country Name2012','Year2008','indicator2008','Country Name2008','Series Code2012','Series Code2008'])\n",
    "\n",
    "\n",
    "predictions_from_model=predictions_from_model.rename(columns={\"predictions2008\": \"2008 [YR2008]\", \"predictions2012\": \"2012 [YR2012]\"})\n",
    "\n",
    "predictions_from_model=predictions_from_model[['2008 [YR2008]','2012 [YR2012]']]\n",
    "\n",
    "predictions_from_model.to_csv(\"./data/result_xboost_0120_05.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CYNTHIA SUGGESTED VISUALIZATION FOR FINAL RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = ('2008 [YR2008]', 2012 [YR2012]')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Predictions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CYNTHIA SUGGESTED TENSORFLOW ALGORITHM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv('result_xboost_0120_03.csv', header=None)\n",
    "#dataframe = predictions_from_model\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    " \n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.mnightly-2021-01-05-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-01-05-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
